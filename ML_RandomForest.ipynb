{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKyniz4qYhvkk5pPBxTseU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phamthien300820/CS114.K21/blob/master/ML_RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cppSi694HWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b2044b66-aebd-459b-c109-967290290598"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LJMvwooixB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d9ddf000-a6b5-47e2-ab2a-edddd580f698"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "!pip install mahotas\n",
        "import mahotas\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.6/dist-packages (1.4.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mahotas) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1trlgUicDeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Đường dẫn output\n",
        "output_path = \"/content/drive/My Drive/X_i1\"\n",
        "\n",
        "# Đường dẫn dữ liệu training\n",
        "train_path = \"/content/drive/My Drive/Machine_Learning Helmet\"\n",
        "\n",
        "# get the training labels\n",
        "train_labels = os.listdir(train_path)\n",
        "train_labels.sort()\n",
        "\n",
        "# fixed-sizes for image\n",
        "fixed_size = tuple((100, 100))\n",
        "\n",
        "# bins for histogram\n",
        "bins = 8\n",
        "\n",
        "# empty lists to hold feature vectors and labels\n",
        "global_features = []\n",
        "labels = []\n",
        "\n",
        "# feature-descriptor-1: Hu Moments\n",
        "def fd_hu_moments(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
        "    return feature\n",
        "\n",
        "# feature-descriptor-2: Haralick Texture\n",
        "def fd_haralick(image):\n",
        "    # convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # compute the haralick texture feature vector\n",
        "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
        "    # return the result\n",
        "    return haralick\n",
        "\n",
        "# feature-descriptor-3: Color Histogram\n",
        "def fd_histogram(image, mask=None):\n",
        "    # convert the image to HSV color-space\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    # compute the color histogram\n",
        "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
        "    # normalize the histogram\n",
        "    cv2.normalize(hist, hist)\n",
        "    # return the histogram\n",
        "    return hist.flatten()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a_wOCoVexJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0129fdf8-407a-472b-9038-e286d1d86ef3"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Có người đi xe máy', 'Không có xe máy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsjZ3SmncKS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "c3d1f42b-947f-4fcd-8119-948a3db243a3"
      },
      "source": [
        "for training_name in train_labels:\n",
        "    \n",
        "    dir = os.path.join(train_path, training_name)\n",
        "    # nhãn đang được train hiện tại\n",
        "    current_label = training_name\n",
        "    # Duyệt tất cả các ảnh trong mỗi tập con\n",
        "    for image in tqdm(os.listdir(dir)):\n",
        "        \n",
        "        try:\n",
        "          path = os.path.join(dir,image)\n",
        "          #Đọc ảnh và resize lại theo fixed_size\n",
        "          image = cv2.imread(path)\n",
        "          image = cv2.resize(image, fixed_size)\n",
        "\n",
        "          ####################################\n",
        "          # Global Feature extraction\n",
        "          ####################################\n",
        "          fv_hu_moments = fd_hu_moments(image)\n",
        "          fv_haralick   = fd_haralick(image)\n",
        "          fv_histogram  = fd_histogram(image)\n",
        "\n",
        "          ###################################\n",
        "          # Concatenate global features\n",
        "          ###################################\n",
        "          global_feature = np.hstack([fv_histogram, fv_hu_moments, fv_haralick])\n",
        "\n",
        "          # update the list of labels and feature vectors\n",
        "          labels.append(current_label)\n",
        "          global_features.append(global_feature)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(\"[STATUS] processed folder: {}\".format(current_label))\n",
        "\n",
        "print(\"[STATUS] completed Global Feature Extraction...\")\n",
        "\n",
        "\n",
        "# get the overall feature vector size\n",
        "print (\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
        "\n",
        "# get the overall training label size\n",
        "print (\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
        "\n",
        "# encode the target labels\n",
        "le = LabelEncoder()\n",
        "target = le.fit_transform(labels)\n",
        "\n",
        "# normalize the feature vector in the range (0-1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_features = scaler.fit_transform(global_features)\n",
        "\n",
        "# save the feature vector using HDF5\n",
        "h5f_data = h5py.File(output_path+'data.h5', 'w')\n",
        "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
        "\n",
        "h5f_label = h5py.File(output_path+'labels.h5', 'w')\n",
        "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
        "\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "\n",
        "print(\"[STATUS] end of training..\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 968/968 [04:09<00:00,  3.88it/s]\n",
            "  0%|          | 0/638 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[STATUS] processed folder: Có người đi xe máy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 638/638 [02:29<00:00,  4.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[STATUS] processed folder: Không có xe máy\n",
            "[STATUS] completed Global Feature Extraction...\n",
            "[STATUS] feature vector size (1542, 532)\n",
            "[STATUS] training Labels (1542,)\n",
            "[STATUS] end of training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuwqNyhvvwrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3028b12f-9dc5-4308-8109-852da2bbc7fe"
      },
      "source": [
        "output_path = \"\\content\\X\"\n",
        "\n",
        "# fixed-sizes for image\n",
        "#fixed_size = tuple((100, 100))\n",
        "\n",
        "# Số lượng cây ở RandomForest\n",
        "num_trees = 300\n",
        "\n",
        "# bins for histogram\n",
        "bins = 8\n",
        "\n",
        "# số hình ảnh mỗi lớp\n",
        "images_per_class = 10;\n",
        "\n",
        "# nhập vector đặc trưng và nhãn được training\n",
        "h5f_data = h5py.File('/content/drive/My Drive/X_i1data.h5', 'r')\n",
        "h5f_label = h5py.File('/content/drive/My Drive/X_i1labels.h5', 'r')\n",
        "\n",
        "global_features_string = h5f_data['dataset_1']\n",
        "global_labels_string = h5f_label['dataset_1']\n",
        "\n",
        "global_features = np.array(global_features_string)\n",
        "global_labels = np.array(global_labels_string)\n",
        "\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "\n",
        "\n",
        "#  Tạo model - Random Forests\n",
        "clf  = RandomForestClassifier(n_estimators=num_trees)\n",
        "clf.fit(global_features, global_labels)\n",
        "\n",
        "# Đường dẫn đến dữ liệu test\n",
        "test_path = \"/content/drive/My Drive/Train_1\"\n",
        "# lấy labels của test\n",
        "test_labels = os.listdir(test_path)\n",
        "\n",
        "# sắp xếp labels của test\n",
        "test_labels.sort()\n",
        "print(test_labels)\n",
        "# loop through the test images\n",
        "test_features = []\n",
        "test_results = []\n",
        "for testing_name in test_labels:\n",
        "    # join the training data path and each species training folder\n",
        "    dir = os.path.join(test_path, testing_name)\n",
        "    # get the current training label\n",
        "    current_label = testing_name\n",
        "    # loop over the images in each sub-folder\n",
        "    for image in tqdm(os.listdir(dir)):\n",
        "        # get the image file name\n",
        "        try:\n",
        "          path = os.path.join(dir,image)\n",
        "          # read the image and resize it oto a fixed-size\n",
        "          image = cv2.imread(path)\n",
        "          image = cv2.resize(image, fixed_size)\n",
        "\n",
        "          ####################################\n",
        "          # Global Feature extraction\n",
        "          ####################################\n",
        "          fv_hu_moments = fd_hu_moments(image)\n",
        "          fv_haralick   = fd_haralick(image)\n",
        "          fv_histogram  = fd_histogram(image)\n",
        "\n",
        "          ###################################\n",
        "          # Concatenate global features\n",
        "          ###################################\n",
        "\n",
        "          # update the list of labels and feature vectors\n",
        "          test_results.append(current_label)\n",
        "          test_features.append(np.hstack([fv_histogram, fv_hu_moments, fv_haralick]))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# predict label of test image\n",
        "le = LabelEncoder()\n",
        "y_result = le.fit_transform(test_results)\n",
        "y_pred = clf.predict(test_features)\n",
        "\n",
        "print(y_pred)\n",
        "print(\"Result: \", (y_pred == y_result).tolist().count(True)/len(y_result))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "  0%|          | 0/182 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['.ipynb_checkpoints', 'Có người đi xe máy', 'Không có xe máy']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [00:26<00:00,  6.80it/s]\n",
            "100%|██████████| 163/163 [00:25<00:00,  6.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1]\n",
            "Result:  0.45692883895131087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTSJpVAP5DRD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "001aa588-72e4-4df3-9481-922d55b7dd65"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_result, y_pred, labels=[0, 1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.12      0.19       140\n",
            "           1       0.46      0.83      0.59       127\n",
            "\n",
            "    accuracy                           0.46       267\n",
            "   macro avg       0.45      0.47      0.39       267\n",
            "weighted avg       0.45      0.46      0.38       267\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}